# Agent Tasks - PF2e Campaign Manager Docker Migration

## Project Overview
Migrating existing PF2e campaign manager from multi-campaign file-based system to Docker-containerized single-campaign instances with web UI and Discord bot integration.

## Current State
- **Working Components:**
  - `campaign_helpers.py` with modern class structures (PlayerCharacter, Campaign, Session, etc.)
  - Character and Campaign data managers using JSON/SQLite
  - PF2e index system generating `pf2e.db`
  - Discord bot functionality
  - Local LLM integration (QwenInstruct)
  - MCP server capabilities

## Target Architecture

### Core Principle
- One Docker container = One active campaign
- Discord bot runs separately, communicates via API
- Web UI for DM management at localhost
- All persistent data stored on host, mounted into containers

### Data Organization
```
~/pf2e-campaigns/
├── campaigns/
│   └── [campaign-name]/
│       ├── campaign.db
│       ├── session-logs/
│       └── character-sheets/
├── shared/
│   ├── pf2e.db       # Generated by pf2e_index
│   └── models/       # LLM model files
└── configs/
    └── discord-bot.env
```

### Container Architecture
```
[Discord Bot Container] ←→ API ←→ [Campaign Container]
                                   ├── Starlette Web UI (localhost:8000)
                                   ├── MCP Server
                                   ├── Society-Scribe (LLM Runtime)
                                   │   ├── Model Loader
                                   │   ├── Inference Engine
                                   │   └── Tool Executor
                                   └── Campaign Manager
```

## Implementation Phases

### Phase 1: Foundation ✅ [CURRENT]
**Goal:** Basic Docker infrastructure and build system

**Tasks:**
- [ ] Create `tasks.py` with invoke commands:
  - `invoke build` - Build Docker image
  - `invoke test` - Run pytest in container
  - `invoke run` - Start campaign container
- [ ] Create `Dockerfile`:
  - Multi-stage build
  - Python 3.11+ base
  - Install model runtimes (without model files)
  - Copy application code
- [ ] Create `docker-compose.yml` template:
  - Campaign container definition
  - Volume mounts for campaign data, pf2e.db, models
  - Environment variables
- [ ] Create `docker-requirements.txt`:
  - Separate from development requirements
  - Include production dependencies only

**Testing:**
- Container builds successfully
- Container can start and access mounted volumes
- Basic Python environment works

### Phase 2: Single Campaign Refactor
**Goal:** Simplify managers to single-campaign operation

**Tasks:**
- [ ] Refactor `CampaignDataManager`:
  - Remove multi-campaign logic
  - Single campaign.db per container
  - Simplify to assume one active campaign
- [ ] Create `src/models/` directory:
  - Move dataclasses from campaign_helpers.py
  - Separate files for Character, Campaign, Session, Equipment
- [ ] Update file structure:
  ```
  src/
  ├── models/
  │   ├── __init__.py
  │   ├── character.py
  │   ├── campaign.py
  │   ├── session.py
  │   └── equipment.py
  └── managers/
      ├── __init__.py
      ├── campaign_manager.py
      └── character_manager.py
  ```
- [ ] Write pytest tests for refactored managers

**Testing:**
- Single campaign CRUD operations work
- Character management within campaign works
- Session tracking works

### Phase 3: Web UI Foundation
**Goal:** Basic Starlette web interface for DM

**Tasks:**
- [ ] Create Starlette application structure:
  ```
  src/web/
  ├── __init__.py
  ├── app.py           # Main Starlette app
  ├── routes/
  │   ├── __init__.py
  │   ├── dm.py        # DM interface routes
  │   └── api.py       # API endpoints
  └── templates/
      └── base.html    # Jinja2 templates
  ```
- [ ] Implement core routes:
  - `/` - Landing page
  - `/dm/dashboard` - Campaign overview
  - `/dm/chat` - Model chat interface
  - `/api/v1/campaign` - Campaign API
- [ ] Add to Docker container
- [ ] Update `invoke run` to expose port 8000

**Testing:**
- Web UI accessible at localhost:8000
- Basic navigation works
- API endpoints respond

### Phase 4: Society-Scribe Integration
**Goal:** Integrate Society-Scribe LLM runtime

**Tasks:**
- [ ] Port Society-Scribe to container environment:
  ```
  src/society_scribe/
  ├── __init__.py
  ├── scribe.py          # Main LLM runtime class
  ├── model_loader.py    # Model loading logic
  ├── inference.py       # Inference engine
  ├── tool_executor.py   # MCP tool execution
  └── prompts/           # System prompts
      ├── dm_assistant.py
      ├── rules_lawyer.py
      └── storyteller.py
  ```
- [ ] Create `SocietyScribe` class:
  - Initialize with model path from mount
  - Handle model loading (GGUF format)
  - Manage inference pipeline
  - Execute MCP tools via tool_executor
- [ ] Integrate with MCP server:
  - Register available tools
  - Handle tool calls from model
  - Return tool results to model
- [ ] Add streaming support for web UI
- [ ] Create invoke commands:
  - `invoke download-model [model-name]`
  - `invoke list-models`
  - `invoke test-inference`
- [ ] Write comprehensive pytest tests

**Testing:**
- Model loads successfully
- Inference produces valid outputs
- Tool execution works through MCP
- Streaming to web UI works
- Memory usage stays within bounds

### Phase 4.5: MCP Server Implementation
**Goal:** Implement proper MCP server for tool exposure

**Tasks:**
- [ ] Create MCP server within Starlette app:
  ```
  src/web/
  ├── app.py           
  ├── routes/
  │   └── mcp.py       # MCP protocol endpoints
  └── mcp/
      ├── __init__.py
      ├── server.py    # MCP server implementation
      ├── registry.py  # Tool registry
      └── tools/       
          ├── campaign.py    # Campaign management
          ├── dice.py        # Dice rolling
          ├── characters.py  # Character sheets
          ├── rules.py       # PF2e rules lookup
          └── session.py     # Session management
  ```
- [ ] Implement MCP protocol:
  - Tool discovery endpoint
  - Tool execution endpoint
  - Result formatting
- [ ] Connect Society-Scribe to MCP:
  - Tool discovery on startup
  - Dynamic tool registration
  - Tool result handling
- [ ] Create test client for MCP validation
- [ ] Write pytest tests for each tool

**Testing:**
- MCP server responds to discovery requests
- Each tool executes correctly
- Society-Scribe can call tools
- Tool results properly formatted
- Integration tests pass

### Phase 5: Discord Bot Separation
**Goal:** Separate Discord bot with API communication

**Tasks:**
- [ ] Create separate Discord bot directory:
  ```
  discord-bot/
  ├── Dockerfile
  ├── bot.py
  └── requirements.txt
  ```
- [ ] Implement campaign API client in bot
- [ ] Add campaign context management
- [ ] Create invoke commands:
  - `invoke start-bot`
  - `invoke stop-bot`
- [ ] Update docker-compose for both services

**Testing:**
- Bot connects to Discord
- Bot can query campaign API
- Commands work through API

### Phase 6: Session Management
**Goal:** Real-time session management via web

**Tasks:**
- [ ] Add WebSocket support to Starlette
- [ ] Create session management UI
- [ ] Implement real-time updates:
  - Dice rolls
  - Character updates
  - Story notes
- [ ] Add session persistence

**Testing:**
- Can start/stop sessions
- Updates appear in real-time
- Session data persists

### Phase 7: Advanced Features
**Goal:** Polish and additional functionality

**Tasks:**
- [ ] Player character sheet access (future)
- [ ] Model hot-swapping UI
- [ ] Campaign backup/restore commands
- [ ] Multi-server Discord support scaffolding
- [ ] Docker health checks
- [ ] Logging and monitoring

## Component Architecture Details

### Society-Scribe (LLM Runtime)
**Purpose:** The actual LLM inference engine that powers all AI interactions

**Responsibilities:**
- Load and manage GGUF format models
- Handle inference requests from web UI and Discord bot
- Execute tool calls through MCP server
- Manage conversation context and memory
- Stream responses for real-time interaction

**Key Design Decisions:**
- Uses llama-cpp-python for efficient inference
- Maintains conversation history per session
- Supports multiple prompt templates (DM assistant, rules lawyer, storyteller)
- Handles tool execution through MCP protocol
- Manages token limits and context windows

**Integration Points:**
- **Web UI:** Direct chat interface, session management
- **MCP Server:** Tool discovery and execution
- **Discord Bot:** API calls for Discord commands
- **Campaign Manager:** Access to campaign state

### MCP Server
**Purpose:** Expose campaign tools to the LLM in a standardized way

**Responsibilities:**
- Implement Model Context Protocol
- Register and manage available tools
- Route tool calls from Society-Scribe to appropriate handlers
- Format tool results for model consumption
- Maintain tool execution audit log

**Available Tools:**
- Campaign management (NPCs, locations, plot points)
- Dice rolling with PF2e rules
- Character sheet operations
- Rules lookup from pf2e.db
- Session state management
- Note-taking and story tracking

### Next Steps:
1. Create `tasks.py` with basic invoke commands
2. Create initial `Dockerfile`
3. Test basic container build and run

## Design Decisions Log

### Decision: Container per Campaign
**Rationale:** Provides complete isolation, simplifies state management, aligns with one-active-campaign constraint.

### Decision: External Model Storage
**Rationale:** Models are large (4-8GB), shared across campaigns, updated independently.

### Decision: Starlette for Web UI
**Rationale:** Lightweight, async-native, good WebSocket support, simple deployment.

### Decision: Separate Discord Bot
**Rationale:** Allows bot persistence while switching campaigns, enables future multi-server support.

## Technical Constraints
- Desktop can only run one LLM at a time
- Currently single Discord server (but design for future multi-server)
- Local deployment only (localhost)
- Python 3.10+ required
- Build environment: Ubuntu 22.04

## Hardware Specifications
- **GPU:** Nvidia GTX 1080ti (11GB VRAM)
- **RAM:** 32GB DDR4
- **CPU:** Intel i7-7700K (4 cores, 8 threads)
- **Implications:** 
  - Can run 7B-13B parameter models comfortably
  - Quantized models (Q4_K_M, Q5_K_M) recommended for optimal performance
  - Single model instance constraint is reasonable

## Testing Strategy

### Continuous Testing Requirements
- **Every phase** must include comprehensive pytest coverage
- **Test structure** mirrors source structure:
  ```
  tests/
  ├── unit/           # Unit tests for individual components
  ├── integration/    # Integration tests between components
  ├── docker/         # Container-specific tests
  └── fixtures/       # Shared test fixtures and data
  ```
- **Coverage target:** Minimum 80% code coverage
- **CI approach:** `invoke test` runs all tests in Docker container
- **Test data:** Use fixtures for consistent test scenarios

### Testing Checklist per Phase
- [ ] Unit tests for new classes/functions
- [ ] Integration tests for component interactions
- [ ] Docker container tests (startup, health, volumes)
- [ ] API endpoint tests (if applicable)
- [ ] MCP tool tests (after Phase 4.5)
- [ ] Performance benchmarks for model operations

## Language Decision: Python vs C++

### Analysis
**Python Advantages:**
- ✅ Existing codebase already in Python
- ✅ Faster development iteration
- ✅ Excellent ecosystem (Discord.py, Starlette, FastAPI, SQLAlchemy)
- ✅ Model libraries (transformers, llama-cpp-python) have mature Python bindings
- ✅ Easier testing and debugging
- ✅ Better for rapid prototyping and feature changes
- ✅ Async/await paradigm works well for Discord bot + web server

**C++ Advantages:**
- ✅ Better performance for compute-intensive operations
- ✅ Lower memory footprint
- ✅ Direct integration with llama.cpp
- ❌ Would require complete rewrite
- ❌ Longer development cycles
- ❌ More complex testing
- ❌ Less mature web framework ecosystem

### Recommendation: Stay with Python
**Rationale:**
1. Performance bottleneck is the LLM inference (handled by optimized C++ via llama-cpp-python)
2. Application logic is I/O bound (Discord, web, database), where Python excels
3. Development velocity is critical for a side project
4. Python's testing ecosystem (pytest) is superior for maintaining quality
5. Can always optimize hot paths with Cython/Rust extensions if needed

**Performance Optimizations in Python:**
- Use async/await throughout for I/O operations
- Leverage llama-cpp-python for efficient model inference
- Use connection pooling for database
- Cache frequently accessed data (pf2e rules)
- Profile and optimize only proven bottlenecks

## Notes for Agent
- Maintain backward compatibility where possible
- Each phase should be independently testable
- Prioritize working features over perfect architecture
- Keep existing class structures, adapt for single-campaign use
- Document any assumptions or blockers encountered